{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4428b348",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6e81ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38e9ee32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2542a1",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05a4632",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import * #TweetTokenizer\n",
    "tokenizer = TweetTokenizer(preserve_case=True, reduce_len=False, strip_handles=True)\n",
    "\n",
    "def Twik_Tweet_Tokenizer(preserve_case=True, reduce_len=False, strip_handles=True):\n",
    "    tokenizer = TweetTokenizer(preserve_case, reduce_len, strip_handles)\n",
    "    \n",
    "Tokenizer_list={\n",
    "    'TweetTokenizer':1,\n",
    "    'WordTokenizer':2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7f7ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets a line as a input and returns a list\n",
    "\n",
    "def Tokenize_line(line, Tokenizer=None):\n",
    "    if Tokenizer == Tokenizer_list['TweetTokenizer']:\n",
    "        return tokenizer.tokenize(line)\n",
    "    elif Tokenizer == Tokenizer_list['WordTokenizer']:\n",
    "        return word_tokenize(line)\n",
    "    else:\n",
    "        return casual_tokenize(line)\n",
    "\n",
    "def Tokenize_lines(lines_list, Tokenizer=None):\n",
    "    return [Tokenize_line(line,Tokenizer) for line in lines_list]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71a9b2f",
   "metadata": {},
   "source": [
    "# Accented Char Remover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa17860a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a7e418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accent_rm_data(line):\n",
    "    return unicodedata.normalize('NFD', line).encode('ascii', 'ignore').decode('ascii')\n",
    "\n",
    "def accent_rm(line):\n",
    "    return unidecode.unidecode(line)\n",
    "\n",
    "def accent_rm_lines(lines_list, remover=1):\n",
    "    if remover == 1:\n",
    "        return [accent_rm_data(line) for line in lines_list]\n",
    "    else:\n",
    "        return [accent_rm(line) for line in lines_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a0ab49",
   "metadata": {},
   "source": [
    "# Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85360d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import krovetzstemmer\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26a1e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = krovetzstemmer.Stemmer()\n",
    "#stemmer = SnowballStemmer(language = ref_file_name ,ignore_stopwords= ignor_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecac603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Stem_SingleToken_list(line):\n",
    "    return [stemmer.stem(words) for words in line]\n",
    "\n",
    "def Stem_MultiToken_list(lines_list):\n",
    "    return [Stem_SingleToken_list(line) for line in lines_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4648a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printSpace(maxvalue, words):\n",
    "    print(\" \"*(maxvalue-(len(words))),end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a9335d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Main_Word\",end=\"\")\n",
    "printSpace(17,\"Main_Words\")\n",
    "print(\"Snowball1\",end=\"\")\n",
    "printSpace(11,\"Snowball1\")\n",
    "print(\"Snowball2\",end=\"\")\n",
    "printSpace(12,\"Snowball2\")\n",
    "print(\"krovetz\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "for words in tokens:\n",
    "    print(words,end=\"\")\n",
    "    printSpace(16,words)\n",
    "    k = stemmer.stem(words)\n",
    "    print(k,end=\"\")\n",
    "    printSpace(11,k)\n",
    "    k = stemmer2.stem(words)\n",
    "    print(k,end=\"\")\n",
    "    printSpace(12,k)\n",
    "    print(stemr.stem(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36843ff6",
   "metadata": {},
   "source": [
    "# HashTag Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c34752",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8499f49f724a0ef665c300ced27244bbe87ce6c95d9f9c779ece4d8a46e5fb13"
  },
  "kernelspec": {
   "display_name": "Tensorflow-normal(2.5.0)",
   "language": "python",
   "name": "tf-normal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
