{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Vscode ENV problems"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We noticed you're using a conda environment. If you are experiencing issues with this environment in the integrated terminal, we recommend that you let the Python extension change \"terminal.integrated.inheritEnv\" to false in your user settings."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Package Installation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# uncomment to install packages for first time\n",
    "'''\n",
    "pip install -r requirements.txt\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# pip install gcld3\n",
    "#\n",
    "'''"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "source": [
    "import num2words"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\n",
    "import os.path\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import csv\n",
    "import codecs\n",
    "import sys\n",
    "import io"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Language Detection"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from langdetect import detect"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Detecting language"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def Detect_Lnaguage_Semantic(line):\n",
    "    try:\n",
    "        return detect(line)\n",
    "    except:\n",
    "        return np.nan"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cleaning Language (semantic)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def Clean_Language_Sem_Singleline(line, language='en'):\n",
    "    return[word for word in line if Detect_Lnaguage_Semantic(word) == language]\n",
    "\n",
    "def Clean_Language_Sem_Multiline(lines_list, language='en'):\n",
    "    return[Clean_Language_Sem_Singleline(line,language = language) for line in lines_list]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cleaning Language (syntatic)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "def clean_non_english(word):\n",
    "    return \"\".join([c for c in word if ord(c) < 128])\n",
    "\n",
    "def Clean_Language_Syn_Singleline(line, language='en'):\n",
    "    return[clean_non_english(word) for word in line]\n",
    "\n",
    "def Clean_Language_Syn_Multiline(lines_list, language='en'):\n",
    "    return[Clean_Language_Syn_Singleline(line,language = language) for line in lines_list]\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## exp"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "source": [
    "#print(clean_non_english('Hello world'))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Hello world\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Removing handle mention"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "USER_REGEX_PATTERN = r\"(@[((A-Za-z0-9).((?<=^)|(?<=)).(?=$)\\w]+)\"\n",
    "\n",
    "def Remove_Mention_Singleline(word, replacer=\" user\"):       \n",
    "    return re.sub(USER_REGEX_PATTERN, replacer, word)\n",
    "\n",
    "def Remove_Mention_Multiline(lines_list, replacer = \" user\"):\n",
    "    return [Remove_Mention_Singleline(line,replacer=replacer) for line in lines_list]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Accented Char Removal"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import unicodedata\n",
    "import unidecode"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def accent_rm_data_Singleline(line):\n",
    "    return unicodedata.normalize('NFD', line).encode('ascii', 'ignore').decode('ascii')\n",
    "\n",
    "def accent_rm_Singleline(line):\n",
    "    return unidecode.unidecode(line)\n",
    "\n",
    "def accent_rm_Multiline(lines_list, remover=1):\n",
    "    if remover == 1:\n",
    "        return [accent_rm_data_Singleline(line) for line in lines_list]\n",
    "    else:\n",
    "        return [accent_rm_Singleline(line) for line in lines_list]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tokenization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "from nltk.tokenize import * #TweetTokenizer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "tokenizer = TweetTokenizer(preserve_case=False, reduce_len=False, strip_handles=False)\n",
    "\n",
    "def Twik_Tweet_Tokenizer(pre_case=True, redu_len=False, strip_handle=True):\n",
    "    global tokenizer\n",
    "    tokenizer = TweetTokenizer(preserve_case=pre_case, reduce_len=redu_len, strip_handles=strip_handle)\n",
    "    \n",
    "Tokenizer_list={\n",
    "    'TweetTokenizer':1,\n",
    "    'WordTokenizer':2\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# gets a line as a input and returns a list\n",
    "\n",
    "def Tokenize_Singleline(line, Tokenizer=None):\n",
    "    if Tokenizer == Tokenizer_list['TweetTokenizer']:\n",
    "        return tokenizer.tokenize(line)\n",
    "    elif Tokenizer == Tokenizer_list['WordTokenizer']:\n",
    "        return word_tokenize(line)\n",
    "    else:\n",
    "        return casual_tokenize(line)\n",
    "\n",
    "def Tokenize_Multiline(lines_list, Tokenizer=None):\n",
    "    return [Tokenize_Singleline(line,Tokenizer) for line in lines_list]\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## exp"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "source": [
    "#print(Tokenize_Singleline(\"@TargetZonePT :pouting_face: no he bloody isn't I was upstairs getting changed !\",1))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['@TargetZonePT', ':pouting_face:', 'no', 'he', 'bloody', \"isn't\", 'I', 'was', 'upstairs', 'getting', 'changed', '!']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# HashTag Segmentation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "from ekphrasis.classes.segmenter import Segmenter"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "segmenter = Segmenter(corpus = \"twitter\")\n",
    "#segmenter = Segmenter(corpus = \"english\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/atanu/anaconda3/envs/tf-normal/lib/python3.8/site-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
      "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def Hashtag_Segmentation_Single(line, replacer =''):\n",
    "  returnlist = []\n",
    "  for word in line:\n",
    "    if word.startswith('#'):\n",
    "      returnlist.append(replacer+segmenter.segment(word[1:]))\n",
    "    else:\n",
    "      returnlist.append(word)\n",
    "  return returnlist\n",
    "\n",
    "##\n",
    "\n",
    "def Hashtag_Segmentation_Multi(lines_list):\n",
    "  return [Hashtag_Segmentation_Single(line) for line in lines_list]\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Lexical Normilization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "def Import_Lexical_Dict(lex_file_dir = 'LexicalNormalizationData.txt'):\n",
    "    lexical_dict = {}\n",
    "    with open(lex_file_dir, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        lexical_dict = {}\n",
    "        for line in lines:\n",
    "            data = line.split()\n",
    "            lexical_dict[data[0]] = data[1]\n",
    "    return lexical_dict"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "lexical_dictionary = Import_Lexical_Dict()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "def  Lexical_Normilization_Singleline(line):\n",
    "    returnlist = []\n",
    "    for word in line:\n",
    "        if word in lexical_dictionary:\n",
    "            returnlist.append(lexical_dictionary[word])\n",
    "        else:\n",
    "            returnlist.append(word)\n",
    "    return returnlist\n",
    "\n",
    "def Lexical_Normilization_Multiline(lines_list):\n",
    "    return [Lexical_Normilization_Singleline(line) for line in lines_list]\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Special Char Removal"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "_STRING_PUNCT = r\"! \\\" # $ % & ' \\( \\) \\* \\+ , - . / : ; < = > \\? @ \\[ \\\\ \\] ^ _ ` \\{ | \\} ~\"\n",
    "_CURRENCY = r\"\\$ £ € ¥ ฿ ₽ ﷼ ₴\"\n",
    "_PUNCTUATION = r\". … , : ; \\! \\? ¿ ؟ ¡ \\( \\) \\[ \\] \\{ \\} < > _ # \\* & 。 ？ ！ ， 、 ； ： ～ · । ، ۔ ؛ ٪ % + - / = @ ^ | ~ \"\n",
    "_QUOTES = r'\\' \" \" ” “ ` ‘ ´ ’ ‚ , „ » « 「 」 『 』 （ ） 〔 〕 【 】 《 》 〈 〉'\n",
    "_NUMBER=r\"0 1 2 3 4 5 6 7 8 9\"\n",
    "_SPECIAL_UNICODE = r\"・ ・ ・ ⁰\"\n",
    "\n",
    "SP_CHAR_DICTIONARY = _CURRENCY + _PUNCTUATION + _QUOTES"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "\n",
    "def MK_SP_Char_dictionary(DataString):\n",
    "    ReturnDict = {}\n",
    "    i=0\n",
    "    for value in DataString:\n",
    "        ReturnDict[value]=i\n",
    "        i+=1\n",
    "    return ReturnDict\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "SP_CHAR_DICTIONARY = MK_SP_Char_dictionary(SP_CHAR_DICTIONARY)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "def Twik_SP_CHAR_STRING(string_punct = False,currency = False,punctuation = False,quotes = False,number = False):\n",
    "    global SP_CHAR_DICTIONARY,_STRING_PUNCT, _CURRENCY, _PUNCTUATION, _QUOTES, _NUMBER\n",
    "    SP_CHAR_DICTIONARY = r''\n",
    "    if string_punct:\n",
    "        SP_CHAR_DICTIONARY+= _STRING_PUNCT\n",
    "    if currency:\n",
    "        SP_CHAR_DICTIONARY+= _CURRENCY\n",
    "    if punctuation:\n",
    "        SP_CHAR_DICTIONARY+= _PUNCTUATION\n",
    "    if quotes:\n",
    "        SP_CHAR_DICTIONARY+= _QUOTES\n",
    "    if number:\n",
    "        SP_CHAR_DICTIONARY+= _NUMBER\n",
    "    #print(SP_CHAR_DICTIONARY)\n",
    "    SP_CHAR_DICTIONARY = MK_SP_Char_dictionary(SP_CHAR_DICTIONARY)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "# input type : list\n",
    "\n",
    "def Special_Char_Remove_Singleline(line, ignor_date_time = True, SP_IGNOR = ['']):\n",
    "    return_list = []\n",
    "    for word in line:\n",
    "        \n",
    "        if word[0] in SP_IGNOR or word in SP_IGNOR:\n",
    "            return_list.append(word)\n",
    "            continue\n",
    "        \n",
    "        if ignor_date_time:\n",
    "            if Check_Date_Time(word):\n",
    "                return_list.append(word)\n",
    "                continue\n",
    "            else:\n",
    "                if word not in SP_CHAR_DICTIONARY and word[0] not in SP_CHAR_DICTIONARY:\n",
    "                    return_list.append(word)\n",
    "        else:\n",
    "            if word and word[0] not in SP_CHAR_DICTIONARY:\n",
    "                    return_list.append(word)\n",
    "    return return_list\n",
    "\n",
    "# input type : list of list\n",
    "\n",
    "def Special_Char_Remove_Multiline(lines_list, ignor_date_time = True):\n",
    "    return [Special_Char_Remove_Singleline(line,ignor_date_time = ignor_date_time) for line in lines_list]\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "def Special_Char_Remove_From_String_Singleline(line, SP_IGNOR = []):\n",
    "    return_string=\"\"\n",
    "    for character in line:\n",
    "        if character not in SP_CHAR_DICTIONARY:\n",
    "            return_string+=character\n",
    "        elif character in SP_IGNOR:\n",
    "            return_string+=character \n",
    "        else:\n",
    "            return_string+=' '\n",
    "    return return_string\n",
    "\n",
    "def Special_Char_Remove_From_String_Multiline(lines_list,SP_IGNOR = [], ignor_date_time = False):\n",
    "    return_list =[]\n",
    "    for line in lines_list:\n",
    "        if ignor_date_time:\n",
    "            if Check_Date_Time(line):\n",
    "                return_list.append(line)\n",
    "                continue\n",
    "            else:\n",
    "                return_list.append(Special_Char_Remove_From_String_Singleline(line, SP_IGNOR = SP_IGNOR))\n",
    "        else:\n",
    "            return_list.append(Special_Char_Remove_From_String_Singleline(line, SP_IGNOR = SP_IGNOR))\n",
    "    return return_list"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Spell Correction"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "#from ekphrasis.classes.spellcorrect import SpellCorrector\n",
    "#from spellchecker import SpellChecker\n",
    "from textblob import Word\n",
    "from textblob import TextBlob"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "#spellcorrector = SpellCorrector(corpus = \"english\") # name --> ekpspc\n",
    "#spellcorrector = SpellChecker() # name --> 'pys'\n",
    "spellcorrector = None # name --> 'txb' "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "# takes a list of words as input\n",
    "def Spell_Correction_Singleline(line, corrector='txb',txb_word=True):\n",
    "    if corrector == 'pys':                                          #pyspellchecker\n",
    "        return [spellcorrector.correction(word) for word in line]  \n",
    "    elif corrector == 'txb':                                        #TextBlob\n",
    "        returnlist = []\n",
    "        for word in line:\n",
    "            if txb_word:\n",
    "                fix = Word(word)\n",
    "                fix = fix.correct()\n",
    "            else:\n",
    "                fix = TextBlob(word)\n",
    "                fix = str(fix.correct())\n",
    "            returnlist.append(fix)\n",
    "        return returnlist\n",
    "    else:\n",
    "        return [spellcorrector.correct(word) for word in line]      #ekphrasis\n",
    "\n",
    "# takes a list of lines as input\n",
    "def Spell_Correction_Multiline(lines_list, corrector = 'txb'):\n",
    "    return [Spell_Correction_Singleline(line,corrector = corrector) for line in lines_list]\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Stemming"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "import krovetzstemmer\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "stemmer = krovetzstemmer.Stemmer()\n",
    "#stemmer = SnowballStemmer(language = ref_file_name ,ignore_stopwords= ignor_stopwords)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "def Stem_SingleToken_list(line):\n",
    "    return [stemmer.stem(words) for words in line]\n",
    "\n",
    "def Stem_MultiToken_list(lines_list):\n",
    "    return [Stem_SingleToken_list(line) for line in lines_list]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "def printSpace(maxvalue, words):\n",
    "    print(\" \"*(maxvalue-(len(words))),end=\"\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Lemmatization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "import nltk\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#nltk.download('wordnet')\n",
    "from nltk import pos_tag"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "# init lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "POS_VALUES = {'r':'v', 'j': 'v', 'n':'n', 'v':'v'}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "def Lemmatize_SingleToken_list(line):\n",
    "    returnlist = []\n",
    "    global POS_VALUES\n",
    "    for words, pos_value in pos_tag(line):\n",
    "        pos = pos_value[0].lower()\n",
    "        if pos not in POS_VALUES:\n",
    "            pos = 'n'\n",
    "        else:\n",
    "            pos = POS_VALUES[pos]\n",
    "        #print(pos)\n",
    "        returnlist.append(lemmatizer.lemmatize(words,pos))\n",
    "    return returnlist\n",
    "\n",
    "def Lemmatize_MultiToken_list(lines_list):\n",
    "    return [Stem_SingleToken_list(line) for line in lines_list]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Stop-Word Removal"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "from nltk.corpus import stopwords"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "# from file\n",
    "def Import_Stop_Word(St_wrd_file_dir = 'indris', from_file=False):\n",
    "    if from_file:\n",
    "        with open(St_wrd_file_dir,'r') as f:\n",
    "            lines = [line.replace('\\n','') for line in f.readlines()]\n",
    "            return set(lines)\n",
    "    else:\n",
    "       return set(stopwords.words(St_wrd_file_dir)) # 'english' , 'indris'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "Stop_Words = Import_Stop_Word(St_wrd_file_dir = 'english',from_file= False) # while importing fromfile use {\"filename.txt\" and from_file=True}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "def rm_Stopword_Singleline(Line_Token):\n",
    "    return_list = []\n",
    "    for line in Line_Token:\n",
    "        for word in line.split():\n",
    "            if word not in Stop_Words:\n",
    "                return_list.append(word)\n",
    "    return return_list\n",
    "\n",
    "def rm_Stopword_Multiline(lines_list):\n",
    "    return[rm_Stopword_Singleline(line) for line in lines_list]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# URL And Email Identification, Expansion/Normalization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "URL_PATTERN = r'https?://\\S+|www\\.\\S+'\n",
    "EXPANSION_ERROR = r'__CONNECTION\\S+|__CLIENT\\S+'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## URL"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Normalization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "def Normalize_urls_Singleline(line, replace_pattern = \"urladd\"):\n",
    "    return[re.sub(pattern= URL_PATTERN, repl = replace_pattern, string = word) for word in line]\n",
    "\n",
    "def Normalize_urls_Multiline(lines_list,replace_pattern = \"urladd\"):\n",
    "    return[Normalize_urls_Singleline(line,replace_pattern = replace_pattern) for line in lines_list]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Expansion"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "import urlexpander"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "def Expand_urls_Singleline(line):\n",
    "    return_list =[]\n",
    "    for word in line:\n",
    "        if re.search(URL_PATTERN,word):\n",
    "            if urlexpander.is_short(word):\n",
    "                return_list.append(re.sub(pattern= EXPANSION_ERROR, repl=\"\",string = urlexpander.expand(word)))\n",
    "            else:\n",
    "                return_list.append(word)\n",
    "        else:\n",
    "            return_list.append(word)\n",
    "    return return_list\n",
    "\n",
    "def Expand_urls_Multiline(lines_list):\n",
    "    return [Expand_urls_Singleline(line) for line in lines_list]\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Email"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "EMAIL_PATTERN = r'^(?:(?!.*?[.]{2})[a-zA-Z0-9](?:[a-zA-Z0-9.+!%-]{1,64}|)|\\\"[a-zA-Z0-9.+!% -]{1,64}\\\")@[a-zA-Z0-9][a-zA-Z0-9.-]+(.[a-z]{2,}|.[0-9]{1,})$'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "def Normalize_emails_Singleline(line, replace_pattern = \"urladd\"):\n",
    "    return[re.sub(pattern= EMAIL_PATTERN, repl = replace_pattern, string = word) for word in line]\n",
    "\n",
    "def Normalize_emails_Multiline(lines_list,replace_pattern = \"urladd\"):\n",
    "    return[Normalize_urls_Singleline(line,replace_pattern = replace_pattern) for line in lines_list]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Expanding Contractions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "import contractions"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "def Expand_Contraction_Singleline(line):\n",
    "    return [contractions.fix(word) for word in line]\n",
    "\n",
    "def Expand_Contraction_Miltiline(lines_list):\n",
    "    return [Expand_Contraction_Singleline(line) for line in lines_list]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Chat Word (Short & Slang Words) Conversion"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "def Import_Chat_Short_and_Slang_Word(file_dir, split_term = ' '):\n",
    "    return_dict = {}\n",
    "    with open(file_dir,'r') as f:\n",
    "        for line in f.readlines():\n",
    "            data = line.split(split_term)\n",
    "            return_dict[data[0].lower()] = (data[1].replace('\\n','')).lower()\n",
    "    return return_dict"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "Chat_Word_Dictionary = Import_Chat_Short_and_Slang_Word('Chat_Word.txt', split_term = '=') #https://github.com/rishabhverma17/sms_slang_translator/blob/master/slang.txt\n",
    "                                                                                           # https://www.webopedia.com/reference/text-abbreviations/"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "def  Chat_Word_Conversion_Singleline(line):\n",
    "    returnlist = []\n",
    "    for word in line:\n",
    "        if word in Chat_Word_Dictionary:\n",
    "            returnlist.append(Chat_Word_Dictionary[word])\n",
    "        else:\n",
    "            returnlist.append(word)\n",
    "    return returnlist\n",
    "\n",
    "def Chat_Word_Conversion_Multiline(lines_list):\n",
    "    return [Lexical_Normilization_Singleline(line) for line in lines_list]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Date Time Checking and Customization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Checking"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "from dateutil.parser import parse"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "def Check_Date_Time(datastring, fuzzy=False):\n",
    "    \"\"\"\n",
    "    Return whether the string can be interpreted as a date.\n",
    "\n",
    "    :param string: str, string to check for date\n",
    "    :param fuzzy: bool, ignore unknown tokens in string if True\n",
    "    \"\"\"\n",
    "    if datastring.isdigit():\n",
    "        return False\n",
    "    try: \n",
    "        parse(datastring, fuzzy=fuzzy)\n",
    "        return True\n",
    "\n",
    "    except ValueError:\n",
    "        return False\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Deletion"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "def Remove_Date_Time_Singleline(line, Fuzzy = False):\n",
    "    return[word for word in line if not Check_Date_Time(word, fuzzy = Fuzzy)]\n",
    "\n",
    "def Remove_Date_Time_Multiline(lines_list, Fuzzy = False):\n",
    "    return[Remove_Date_Time_Singleline(line, Fuzzy = Fuzzy) for line in lines_list]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Number to Word"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "from num2words import num2words"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "__NUMBERS_= ['0','1','2','3','4','5','6','7','8','9']\n",
    "\n",
    "def isdigit_func(word):\n",
    "    for chr in word:\n",
    "        if chr not in __NUMBERS_:\n",
    "            return False\n",
    "    return True"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "def Number_to_Word_Singleline(line, replacer = \"\"):\n",
    "    return_list = []\n",
    "    for word in line:\n",
    "        if isdigit_func(word):\n",
    "            #print(word)\n",
    "            return_list.append(num2words(int(word)).replace('-',replacer))\n",
    "        else:\n",
    "            return_list.append(word)\n",
    "    return return_list\n",
    "\n",
    "def Number_to_Word_Multiline(lines_list, replacer = \"\"):\n",
    "    return [Number_to_Word_Singleline(line, replacer=replacer) for line in lines_list]\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## exp"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "source": [
    "#print(num2words(int('96')).replace('-',' '))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Emojis and Emoticons to Text Conversion and Deletion"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "import emot"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "emo_converter = emot.emot()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Emo -ji/ticons Text Conversion"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "def Translate_Emoticons(word):\n",
    "    data = emo_converter.emoticons(word)\n",
    "    if data['flag']:\n",
    "        word = data['mean'][0]\n",
    "    return [data['flag'],word]\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "EXTRA_EMOTICONS = set([':p'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "def Emoji_Emoticon_to_Text_Singleline(line, check_date_time = False):\n",
    "    return_list = []\n",
    "    data = []\n",
    "    #print(line)\n",
    "    for word in line:\n",
    "        if not word:\n",
    "            continue\n",
    "        if check_date_time:\n",
    "            if str(word[0]).isdigit():\n",
    "                if Check_Date_Time(word,fuzzy=True):\n",
    "                    return_list.append(word)\n",
    "                    continue\n",
    "\n",
    "        if word in EXTRA_EMOTICONS:\n",
    "            data = Translate_Emoticons(word.upper())\n",
    "        else:\n",
    "            data = Translate_Emoticons(word)\n",
    "            \n",
    "        if data[0]:\n",
    "            return_list.append(data[1])\n",
    "        else:\n",
    "            data = emo_converter.emoji(word)\n",
    "            \n",
    "            if data['flag']:\n",
    "                return_list.append(data['mean'][0].replace(':',''))\n",
    "            else:\n",
    "                return_list.append(word)\n",
    "    return return_list\n",
    "\n",
    "def Emoji_Emoticons_to_Text_Multiline(lines_list):\n",
    "    return[Emoji_Emoticon_to_Text_Singleline(line) for line in lines_list]\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Removing Emoji-Emoticons"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "def Emoji_Emoticon_Remove_Singleline(line, check_date_time = False):\n",
    "    return_list = []\n",
    "\n",
    "    for word in line:\n",
    "        if check_date_time:\n",
    "            if str(word[0]).isdigit():\n",
    "                if Check_Date_Time(word,fuzzy=True):\n",
    "                    return_list.append(word)\n",
    "                    continue\n",
    "\n",
    "        data = emot.emoticons(word)\n",
    "        check =False\n",
    "\n",
    "        if type(data) == type([]):\n",
    "            check = data[0]['flag']\n",
    "        else:\n",
    "            check = data['flag']\n",
    "\n",
    "        if check:\n",
    "            continue\n",
    "        else:\n",
    "            data = emot.emoji(word)\n",
    "            \n",
    "            if data['flag']:\n",
    "                continue\n",
    "            else:\n",
    "                return_list.append(word)\n",
    "    return return_list\n",
    "\n",
    "def Emoji_Emoticons_Remove_Multiline(lines_list):\n",
    "    return[Emoji_Emoticon_to_Text_Singleline(line) for line in lines_list]\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Text to Emoji/Emoticons conversion"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from emot import EMOTICONS_EMO,EMOJI_UNICODE"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "#only emojis\n",
    "\n",
    "def Text_to_Emoji_Emoticons_Singleline(line):\n",
    "    line = line.split()\n",
    "    returnstring = \"\"\n",
    "    for words in line:\n",
    "        if words in EMOJI_UNICODE:\n",
    "            returnstring+=\" \"+EMOJI_UNICODE[words]\n",
    "        else:\n",
    "            returnstring+=\" \"+words\n",
    "    return returnstring"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## exp"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#print(Emoji_Emoticon_to_Text_Singleline([\":p\"], check_date_time = False))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Removing Extra Spaces and Single Character"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "import re\n",
    "SPACE_PATTERN = r' +'#r'\\s+'\n",
    "SINGLE_CHAR_RM = r\"((?<=^)|(?<= )).((?=$)|(?= ))\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "#input -> String\n",
    "def Remove_Extra_Spaces_and_char_Singleline(line, RM_SINGLE_CHAR = False):\n",
    "    if RM_SINGLE_CHAR:\n",
    "        return re.sub(SPACE_PATTERN, \" \", re.sub(SINGLE_CHAR_RM, '', line).strip())\n",
    "    return re.sub(pattern = SPACE_PATTERN, repl =\" \", string = line)\n",
    "\n",
    "#input -> list of string\n",
    "def Remove_Extra_Spaces_and_char_Multiline(lines_list):\n",
    "    return[Remove_Extra_Spaces_and_char_Singleline(line) for line in lines_list]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Removing Repeated Words"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "def RM_Repeated_Word_Singleline(line):\n",
    "    return ' '.join(dict.fromkeys(line.split()))\n",
    "    \n",
    "def RM_Repeated_Word_Multilineline(lines_list):\n",
    "    return[RM_Repeated_Word_Singleline(line) for line in lines_list]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing Module"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## twiking"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "#Twik_SP_CHAR_STRING(string_punct = False,currency = True,punctuation = True,quotes = True,number = True)\n",
    "Twik_Tweet_Tokenizer(pre_case=True, redu_len=True, strip_handle=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## processing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "SP_IGNOR_LIST = [''] #\"'\"\n",
    "\n",
    "def PreProcessingModule(line):\n",
    "\n",
    "    line = accent_rm_data_Singleline(line) # in/out -> string\n",
    "    #print(\"Accent removal: \",line)\n",
    "\n",
    "    line = Remove_Mention_Singleline(line) # in/out -> string\n",
    "    #print(\"Removing Mention: \",line)\n",
    "\n",
    "    line = Text_to_Emoji_Emoticons_Singleline(line) # in/out -> string\n",
    "    #print(\"EMoji Emoticon: \",line)\n",
    "    \n",
    "    line = Expand_Contraction_Singleline([line]) #in/out ->list with only one element\n",
    "    #print(\"Expand Contraction: \", line)\n",
    "\n",
    "\n",
    "    line = line[0] # in->list with only one element // out-> string\n",
    "    #print(line)\n",
    "\n",
    "    line = Tokenize_Singleline(line,1) #in/out -> list\n",
    "    #print(\"Tokenization: \", line)\n",
    "\n",
    "    line = Normalize_urls_Singleline(line, replace_pattern = 'url')    \n",
    "    #print(\"URL normalization: \",line)\n",
    "        \n",
    "    '''\n",
    "    Text_blob_Word = False\n",
    "    if Text_blob_Word:\n",
    "        line = Spell_Correction_Singleline(line)\n",
    "    else:\n",
    "        line = [\" \".join(line)]\n",
    "        line = Spell_Correction_Singleline(line,txb_word=False)\n",
    "        #print(line)\n",
    "        line = Tokenize_Singleline(line[0],1)\n",
    "    '''\n",
    "    #print(line)\n",
    "\n",
    "    line = Hashtag_Segmentation_Single(line, replacer ='')\n",
    "    #print(\"Hashtag Segmentation: \",line)\n",
    "\n",
    "    line = Emoji_Emoticon_to_Text_Singleline(line,check_date_time=True)\n",
    "    #print(\"Emoji Emoticon to Text: \",line)\n",
    "\n",
    "    line = Special_Char_Remove_Singleline(line, SP_IGNOR = [''])\n",
    "    #print(\"Special char Removal: \",line)\n",
    "\n",
    "    line = Lexical_Normilization_Singleline(line)\n",
    "    #print(\"Lexical Normalization: \", line)\n",
    "\n",
    "    line = Chat_Word_Conversion_Singleline(line)\n",
    "    #print(line)\n",
    "\n",
    "    line = Special_Char_Remove_From_String_Multiline(line, SP_IGNOR = SP_IGNOR_LIST, ignor_date_time = False)\n",
    "    #print(line)\n",
    "\n",
    "    #line = Remove_Extra_Spaces_Multiline(line)\n",
    "    #print(line)\n",
    "\n",
    "    line = \" \".join(line).split()\n",
    "\n",
    "\n",
    "    line = Number_to_Word_Singleline(line, replacer =\" \")\n",
    "    #print(\"Number to word: \",line)\n",
    "\n",
    "    line = Lemmatize_SingleToken_list(line)\n",
    "\n",
    "    line = Stem_SingleToken_list(line)\n",
    "    #print(\"Stemming: \",line)\n",
    "\n",
    "    line = Lemmatize_SingleToken_list(line)\n",
    "    #print(line)\n",
    "    \n",
    "    line = rm_Stopword_Singleline(line)\n",
    "    #print(line)\n",
    "    \n",
    "    #line = Remove_Date_Time_Singleline(line)\n",
    "    #print(line)\n",
    "\n",
    "    line = Remove_Extra_Spaces_and_char_Singleline(\" \".join(line), RM_SINGLE_CHAR = True)#.split()\n",
    "\n",
    "    line = RM_Repeated_Word_Singleline(line)\n",
    "    \n",
    "    return line"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "source": [
    "DATA_DIRECTORY = '''../../DataSets/SemEval2018-IronyDetection/SemEval2018-IronyDetection.txt'''  #SmallVersion"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "TweetsTextData = []\n",
    "Rawline = []\n",
    "with open(DATA_DIRECTORY, 'r') as f:\n",
    "    next(f)\n",
    "    lines = [line.split('\\t') for line in f]\n",
    "    global TweetsTextData, Rawline\n",
    "    TweetsTextData=[]\n",
    "    Rawline = []\n",
    "    PreProcessedData = []\n",
    "    i=0\n",
    "    #for line in tqdm(csv.reader(lines,dialect = \"excel-tab\"), total=len(lines)):\n",
    "    for line in tqdm(lines, total=len(lines)):\n",
    "        #i+=1\n",
    "        #print(i)\n",
    "        #try:\n",
    "        #print(\"before:\",line[2])\n",
    "        PreProcessedData =PreProcessingModule((line[2]))\n",
    "        Rawline.append(line[2])\n",
    "        TweetsTextData.append(PreProcessedData)\n",
    "        #print(\"after: \",PreProcessedData)\n",
    "        #except:\n",
    "         #   print(line[2])\n",
    "          #  print(PreProcessedData)\n",
    "        #print()\n",
    "        #TweetsTextData.append(PreProcessedData)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 3834/3834 [00:07<00:00, 483.75it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "del f"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# test"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(Tokenize_Singleline('\"@myrcurial: @amz__123 Aaaaaamd what time is your bedtime? >better  tweet Dad at night from my house. I don\\'t want to get in trouble!',1))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(Rawline[3027])\n",
    "print(TweetsTextData[3027])\n",
    "#print(len(TweetsTextData))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(Special_Char_Remove_Singleline(['Aaaamd'], SP_IGNOR = ['']))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "testline = '''\"@myrcurial: @amz__123 Aaaaaamd what time is your bedtime? >better  tweet Dad at night from my house. I don\\'t want to get in trouble!'''\n",
    "print(testline)\n",
    "#print(Tokenize_Singleline(testline,1))\n",
    "print(PreProcessingModule(testline))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Main Module"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8499f49f724a0ef665c300ced27244bbe87ce6c95d9f9c779ece4d8a46e5fb13"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('tf-normal': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}